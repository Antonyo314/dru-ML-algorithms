{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "code_folding": [
     0,
     12,
     21,
     30,
     44
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_prediction(w, X):\n",
    "    \"\"\"\n",
    "    Returns a linear regression model prediction labels for objects in matrix \n",
    "    X using weights w:\n",
    "    y_pred = (X,w)\n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        return(np.insert(X, 0, 1).dot(w))\n",
    "    else:\n",
    "        n = X.shape[0]\n",
    "        return np.dot(np.hstack((np.ones((n,1)),X)),w)\n",
    "\n",
    "def mean_squared_error(y, y_pred):\n",
    "    \"\"\"\n",
    "    Returns a mean squared error between real and predicted labels\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mse = np.sum((y - y_pred)**2) / y.size\n",
    "    return mse\n",
    "\n",
    "def mean_absolute_error(y, y_pred):\n",
    "    \"\"\"\n",
    "    Returns a mean absolute error between real and predicted labels\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mae = np.sum(abs(y - y_pred)) / y.size\n",
    "    return mae\n",
    "\n",
    "def cost_function(w, X, y, type_f='mse'):\n",
    "    \"\"\"\n",
    "    Returns a cost function of a linear model with coefficients w\n",
    "    oh features X and labels y.\n",
    "    \"\"\"\n",
    "    if (type_f == 'mae'):\n",
    "        return mean_absolute_error(y, linear_prediction(w, X))\n",
    "    elif (type_f == 'mse'):\n",
    "        return mean_squared_error(y, linear_prediction(w, X))\n",
    "    else:\n",
    "        #raise\n",
    "        print('error: incorrect type of function')\n",
    "        return\n",
    "\n",
    "def linear_regression_fit(X, y, minimize='grad_desc', cost_f='mse',\n",
    "                          w0=None, eta=1e-2):\n",
    "    \"\"\"\n",
    "    Returns weights that minimizes cost function.\n",
    "    \"\"\"   \n",
    "    if (minimize == 'analytical'):\n",
    "        # the cost function is 'mse' automatically\n",
    "        # X_t = X.transpose()\n",
    "        # w = np.linalg.inv(X_t.dot(X)).dot(X_t).dot(y)\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n,1)),X))\n",
    "        w = np.linalg.lstsq(X,y)[0]\n",
    "        \n",
    "    elif (minimize == 'grad_desc'):\n",
    "        # the cost function is 'mse' automatically\n",
    "        if(w0 == None):\n",
    "            w0 = np.zeros(X.shape[1] + 1)\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n,1)),X))\n",
    "        w = gradient_descent(X, y, w0, eta)[0]\n",
    "        w = w.reshape(w.shape[0])\n",
    "\n",
    "    elif (minimize == 'st_grad_desc'):\n",
    "        # the cost function is 'mse' automatically\n",
    "        if(w0 == None):\n",
    "            w0 = np.zeros(X.shape[1] + 1)\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n,1)),X))\n",
    "        w = stochastic_gradient_descent(X, y, w0, eta)[0]\n",
    "        w = w.reshape(w.shape[0])\n",
    "    \n",
    "    elif (minimize == 'scipy_minimize'):\n",
    "        if(w0 == None):\n",
    "            w0 = np.zeros(X.shape[1] + 1)\n",
    "        w = scipy.optimize.minimize(lambda w: cost_function(w, X, y, cost_f), w0).x\n",
    "    else:\n",
    "        #raise\n",
    "        print('error: incorrect minimization method')\n",
    "        return\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "code_folding": [
     0,
     5
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_step(X, y, w, eta=0.01):\n",
    "    grad = 2 * X.T.dot(X.dot(w) - y) / X.shape[0] \n",
    "    w_next = w - eta * grad \n",
    "    return w_next\n",
    "\n",
    "def gradient_descent(X, y, w0=None, eta=1e-2,\n",
    "                     max_iter=1e4, min_weight_dist=1e-8):    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if(w0 == None):\n",
    "        w0 = np.zeros(X.shape[1])\n",
    "        \n",
    "    w = w0\n",
    "    weight_dist = np.inf\n",
    "    errors = []\n",
    "    iter_num = 0\n",
    "        \n",
    "    while weight_dist > min_weight_dist and iter_num < max_iter: \n",
    "        w_next = gradient_step(X, y, w, eta)        \n",
    "        errors.append(mean_squared_error(y, X.dot(w_next)))\n",
    "        weight_dist = np.linalg.norm(w_next - w)      \n",
    "        w = w_next\n",
    "        iter_num += 1\n",
    "    return w, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_step(X, y, w, k, eta=0.01):\n",
    "    grad = 2 * (X[k].dot(w) - y[k]) * X[k,:] / X.shape[0] \n",
    "    w_next = w - eta * grad   \n",
    "    return w_next\n",
    "\n",
    "def stochastic_gradient_descent(X, y, w0=None, eta=1e-2, max_iter=1e4,\n",
    "                                min_weight_dist=1e-8, seed=42):    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if(w0 == None):\n",
    "        w0 = np.zeros(X.shape[1])\n",
    "        \n",
    "    w = w0\n",
    "    weight_dist = np.inf\n",
    "    errors = []\n",
    "    iter_num = 0\n",
    "    #import random\n",
    "    #k = np.array(range(X.shape[0]))\n",
    "    #random.shuffle(k)\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    while weight_dist > min_weight_dist and iter_num < max_iter:  \n",
    "        #print('\\nit.',iter_num)\n",
    "        #print('w: ',w)\n",
    "        random_ind = np.random.randint(X.shape[0])\n",
    "        w_next = stochastic_gradient_step(X, y, w, random_ind, eta)     \n",
    "        errors.append(mean_squared_error(y, X.dot(w_next)))\n",
    "        weight_dist = np.linalg.norm(w_next - w)      \n",
    "        #print('d: ',weight_dist)\n",
    "        w = w_next\n",
    "        iter_num += 1\n",
    "    return w, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_result(coef, true, predict, cut = 5):\n",
    "    print('w:\\n',coef,'\\n')\n",
    "    print('true vs. prediction:\\n',np.vstack((true,predict)).T[:cut],'\\n...')\n",
    "    print('root mean squared error: ',round((mean_squared_error(true, predict))**.5,3))\n",
    "    print('mean absolute error: ',round(mean_absolute_error(true, predict),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_for_one_feature(train_data, train_labels, w, title):\n",
    "    x = np.linspace(train_data.min(), train_data.max(), 2).reshape((2,1))\n",
    "    plt.figure(figsize = (8,5))\n",
    "    plt.plot(train_data, train_labels, 'o', markersize = 3)\n",
    "    #print(x)\n",
    "    #print(linear_prediction(w, x))\n",
    "    plt.plot(x, linear_prediction(w,x), '-', linewidth = 4)\n",
    "    plt.xlabel('feature')\n",
    "    plt.ylabel('label')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(X, mean_std=True):\n",
    "    if mean_std:\n",
    "        means, stds = X.mean(axis=0), X.std(axis=0)\n",
    "        X = (X - means) / stds\n",
    "    else:\n",
    "        minim, maxim = data.min(axis = 0), data.max(axis = 0)\n",
    "        X = (X - minim) / (maxim - minim)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "\n",
    "sample_size = 200\n",
    "data, target = datasets.make_regression(n_samples = sample_size,\n",
    "                                        n_features = 1, \n",
    "                                        n_informative = 1, \n",
    "                                        n_targets = 1, noise = 5.,\n",
    "                                        coef = False, random_state = 2)\n",
    "#data = normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \"Advertising\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('Data/weights_heights.csv', index_col='Index')\n",
    "data_frame = pd.read_csv('Data/advertising.csv')\n",
    "\n",
    "features = ['TV'] # , 'Radio','Newspaper']\n",
    "labels = ['Sales']\n",
    "data = np.array(data_frame[features].values, dtype=float)[:sample_size]\n",
    "target = np.array(data_frame[labels].values, dtype=float)[:sample_size]\n",
    "target = target.reshape(target.shape[0])\n",
    "\n",
    "data = normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy dataset \"Boston\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_boston()\n",
    "data = dataset.data\n",
    "target = dataset.target\n",
    "\n",
    "data = normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train & test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cross_val\n",
    "\n",
    "train_data, test_data, \\\n",
    "train_labels, test_labels = cross_val.train_test_split(data, target,\n",
    "                                                       test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: \n",
      " [[-0.40766012  0.97154295 -0.73637217 -0.27259857 -1.05124209  0.2996992\n",
      "  -1.78424818  0.80653853 -0.29308074 -0.47061187 -1.08911039  0.29533561\n",
      "  -0.55832104]\n",
      " [-0.40354288 -0.48772236 -0.37597609 -0.27259857 -0.29970737  0.26978136\n",
      "   1.01436883 -0.6475206  -0.52300145 -0.14395131  1.13022958  0.4228511\n",
      "  -0.05369542]\n",
      " [ 1.07222115 -0.48772236  1.01599907 -0.27259857  1.60072524 -0.61350702\n",
      "   0.99658854 -0.90293643  1.66124525  1.53092646  0.80657583 -1.27355443\n",
      "   1.56110656]\n",
      " [-0.20492927 -0.48772236  1.2319449   3.66839786  0.43455068  2.16172808\n",
      "   1.05348546 -0.83396037 -0.52300145 -0.03110494 -1.73641788  0.36112176\n",
      "  -1.50449408]\n",
      " [-0.41115675 -0.48772236  0.11573841 -0.27259857  0.15812412  0.43931575\n",
      "   0.01867281 -0.62579623 -0.98284286 -0.80321172  1.17646583  0.38721693\n",
      "  -0.41814726]] \n",
      "...\n",
      "\n",
      "train_labels: \n",
      " [ 26.4  19.8  10.8  50.   22.4] ...\n"
     ]
    }
   ],
   "source": [
    "print('train_data: \\n',train_data[:5],'\\n...\\n')\n",
    "print('train_labels: \\n',train_labels[:5],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical OLS method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 22.44376062  -0.49832497   1.1037074    0.36817348   1.12305293\n",
      "  -1.86223667   3.39902492  -0.30222417  -2.83374334   2.1979636\n",
      "  -2.03170579  -1.85956253   0.93330698  -3.16650413] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         18.82826868]\n",
      " [ 22.          26.08788447]\n",
      " [ 13.4         16.39740984]\n",
      " [ 30.8         31.26097619]\n",
      " [ 24.3         23.43286557]] \n",
      "...\n",
      "root mean squared error:  5.448\n",
      "mean absolute error:  3.546\n"
     ]
    }
   ],
   "source": [
    "w = linear_regression_fit(train_data, train_labels, minimize='analytical')\n",
    "print_result(w,test_labels,linear_prediction(w, test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical method using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 22.44376062  -0.49832493   1.10370736   0.36817327   1.12305296\n",
      "  -1.86223661   3.39902497  -0.30222419  -2.83374335   2.19796316\n",
      "  -2.03170529  -1.8595625    0.93330697  -3.16650412] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         18.82826874]\n",
      " [ 22.          26.08788453]\n",
      " [ 13.4         16.39740995]\n",
      " [ 30.8         31.26097613]\n",
      " [ 24.3         23.43286543]] \n",
      "...\n",
      "root mean squared error:  5.448\n",
      "mean absolute error:  3.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yaroslava/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "w0 = np.array([1,0])\n",
    "w = linear_regression_fit(train_data, train_labels,\n",
    "                          minimize='grad_desc', eta=0.1)\n",
    "\n",
    "print_result(w,test_labels,linear_prediction(w, test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical method using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yaroslava/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 21.05828054  -0.44114324   0.90649008  -0.39014135   1.02538009\n",
      "  -0.86013865   3.59852964   0.1829777   -1.66467708   0.34865592\n",
      "  -0.42275516  -1.71221661   0.66769586  -2.78834143] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         18.51560566]\n",
      " [ 22.          24.85790143]\n",
      " [ 13.4         16.76402607]\n",
      " [ 30.8         28.65614294]\n",
      " [ 24.3         21.26785382]] \n",
      "...\n",
      "root mean squared error:  5.874\n",
      "mean absolute error:  3.756\n"
     ]
    }
   ],
   "source": [
    "w0 = np.array([1,0])\n",
    "w = linear_regression_fit(train_data, train_labels,\n",
    "                          minimize='st_grad_desc', eta=0.05)\n",
    "\n",
    "print_result(w,test_labels,linear_prediction(w, test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical method using scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 21.57250514  -0.25650434   0.61159515   0.21744535   0.38726468\n",
      "  -0.81356805   3.97613022  -0.70018654  -1.83983897   1.08398396\n",
      "  -1.79501132  -1.61558548   1.0326189   -2.54325451] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         18.83540185]\n",
      " [ 22.          25.86114687]\n",
      " [ 13.4         14.83382662]\n",
      " [ 30.8         29.77006751]\n",
      " [ 24.3         22.13718297]] \n",
      "...\n",
      "root mean squared error:  5.706\n",
      "mean absolute error:  3.508\n"
     ]
    }
   ],
   "source": [
    "w0 = np.array([1,0])\n",
    "w = linear_regression_fit(train_data, train_labels, minimize='scipy_minimize', cost_f='mae')\n",
    "\n",
    "w.reshape((1,w.shape[0]))\n",
    "print_result(w,test_labels,linear_prediction(w, test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.linear_model.SGDRegression for check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " [ 22.44025172  -0.32870933   0.91243119  -0.04248737   1.21194893\n",
      "  -1.47927849   3.54029591  -0.20553943  -2.55304485   1.26651369\n",
      "  -0.98148952  -1.73279411   0.91530178  -3.15202024] \n",
      "\n",
      "true vs. prediction:\n",
      " [[ 18.2         19.16797719]\n",
      " [ 22.          26.28505887]\n",
      " [ 13.4         17.20411417]\n",
      " [ 30.8         30.52080577]\n",
      " [ 24.3         22.95317484]] \n",
      "...\n",
      "root mean squared error:  5.527\n",
      "mean absolute error:  3.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yaroslava/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model \n",
    "\n",
    "sgd_regressor = linear_model.SGDRegressor(random_state=None, n_iter=20)\n",
    "sgd_regressor.fit(train_data, train_labels)\n",
    "sgd_regressor.predict(test_data)\n",
    "w = [sgd_regressor.intercept_[0]]\n",
    "w.extend(sgd_regressor.coef_)\n",
    "w = np.array(w)\n",
    "print_result(w,test_labels,sgd_regressor.predict(test_data))\n",
    "if train_data.shape[1] == 1:\n",
    "    #plot_for_one_feature(train_data, train_labels, w, 'train')\n",
    "    plot_for_one_feature(test_data, test_labels, w, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
